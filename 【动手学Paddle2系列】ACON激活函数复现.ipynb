{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ACON激活函数复现\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3c3ae675554c4c949aaf6222326550107568506ac3384080b022663b48969f03\"></center>\n",
    "<br></br>\n",
    "\n",
    "&emsp;&emsp;在此论文中作者提出了一个简单、有效的激活函数ACON，该激活函数可以决定是否要激活神经元，在ACON基础上作者进一步提出了激活函数，它通过引入开关因子去学习非线性（激活）和线性（非激活）之间的参数切换。实验结果表明，在图像分类，目标检测以及语义分割的任务上，该激活函数都可以使得深度模型有显著的提升效果。\n",
    "\n",
    "\n",
    "\n",
    "[论文地址](https://arxiv.org/abs/2009.04759) [代码地址](https://github.com/nmaac/acon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Smooth Maximum（光滑最大值函数）\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;我们目前常用的激活函数本质上都是MAX函数，以ReLU函数为例，其形式可以表示为：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0915cc5b0abe4526a162ac83c4bf380aef53002d35914eabbba1a63ae33ec5e0\"></center>\n",
    "<br></br>\n",
    "\n",
    "而MAX函数的平滑，可微分变体我们称为Smooth Maximum，其公式如下：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9b3cb6dc975946b889d949404827e8028258c6f382e1413fa151c74da6946f8a\"></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "这里我们只考虑Smooth Maximum只有两个输入量的情况，即n=2，于是有以下公式：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6f7ba323c5f34205b9e230e54f0de47101388f54944247debabf1fc59361ea2c\"></center>\n",
    "<br></br>\n",
    "\n",
    "考虑平滑形式下的ReLU![](https://ai-studio-static-online.cdn.bcebos.com/4e5a2b7b626d4e1d8d119b537a651facb2a9e5a0c7cf4ad791416e55df7a885a)，代入公式我们得到而这个结果![](https://ai-studio-static-online.cdn.bcebos.com/91d853a5d25b46558f73469d821ce01ca27f89cf4e824c7ca7a1822467607287)就是Swish激活函数！所以我们可以得到，Swish激活函数是ReLU函数的一种平滑近似。我们称其为ACON-A：\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8d099a4825a34b5e8f7769eadec68eefa6b15b8423434496849f4c954d28e800\"></center>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/1b3c3236d9b54f5eb42ef3ab675122d56fd21b60ffac47f6b207fe499969fc1f\"></center>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b2ab1f5b070540f4b9efaf81524c40cd044b66dd34c448599d26e62cffca05cb\"></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "ACON-C的一阶导数计算公式如下所示：\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/53c6d63f17e14e6abc8156dde49d667e18d049e2a0e745128533dbf83a422e4b\"></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "\n",
    "解上述方程可得：\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/1d23a25b348a4b4397ee040ea3c2c8a0bb9c0ef3c07148b7a96d895f5170e912\"></center>\n",
    "<br></br>\n",
    "\n",
    "可学习的边界对于简化优化是必不可少的，这些可学习的上界和下届是改善结果的关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 基于飞桨框架的复现\n",
    "\n",
    "\n",
    "\n",
    "* **一些API**介绍\n",
    "\n",
    "\n",
    "`paddle.static.create_parameter`\n",
    "\n",
    "\n",
    "该OP创建一个参数。该参数是一个可学习的变量, 拥有梯度并且可优化。\n",
    "\n",
    "\n",
    "根据ACON的官方代码，我复现了Paddle版本的ACON-C如下所示。对比下来，基于飞桨框架的API更加简练一点，并且可以直接在API里指定初始化方式。实际上，各种初始化方式也有很多的，大家可以自行百度一下哦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle import nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle import ParamAttr\r\n",
    "from paddle.regularizer import L2Decay\r\n",
    "from paddle.nn import AvgPool2D, Conv2D\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "class AconC(nn.Layer):\r\n",
    "    \"\"\" ACON activation (activate or not).\r\n",
    "    # AconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is a learnable parameter\r\n",
    "    # according to \"Activate or Not: Learning Customized Activation\" <https://arxiv.org/pdf/2009.04759.pdf>.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, width):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.p1 = paddle.create_parameter([1, width, 1, 1], dtype='float32', default_initializer=nn.initializer.Normal())\r\n",
    "        self.p2 = paddle.create_parameter([1, width, 1, 1], dtype='float32', default_initializer=nn.initializer.Normal())\r\n",
    "        self.beta = paddle.create_parameter([1, width, 1, 1], dtype='float32', default_initializer=paddle.fluid.initializer.NumpyArrayInitializer(np.ones([1, width, 1, 1])))\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return (self.p1 * x - self.p2 * x) * F.sigmoid(self.beta * (self.p1 * x - self.p2 * x)) + self.p2 * x\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dcn2(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(dcn2, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding = 1)\r\n",
    "        # self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3),  stride=2, padding = 0)\r\n",
    "        # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "\r\n",
    "        self.acon1 = AconC(64)\r\n",
    "      \r\n",
    "\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool2(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = self.acon1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        \r\n",
    "        # offsets = self.offsets(x)\r\n",
    "        # masks = self.mask(x)\r\n",
    "        # print(offsets.shape)\r\n",
    "        # print(masks.shape)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 网络结构可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1      [[64, 3, 32, 32]]     [64, 32, 32, 32]         896      \n",
      "   Conv2D-2      [[64, 32, 32, 32]]    [64, 64, 15, 15]       18,496     \n",
      "   Conv2D-3      [[64, 64, 15, 15]]     [64, 64, 7, 7]        36,928     \n",
      "    AconC-1       [[64, 64, 7, 7]]      [64, 64, 7, 7]          192      \n",
      "   Conv2D-4       [[64, 64, 7, 7]]      [64, 64, 4, 4]        36,928     \n",
      "   Flatten-1      [[64, 64, 4, 4]]        [64, 1024]             0       \n",
      "   Linear-1         [[64, 1024]]           [64, 64]           65,600     \n",
      "   Linear-2          [[64, 64]]            [64, 1]              65       \n",
      "===========================================================================\n",
      "Total params: 159,105\n",
      "Trainable params: 159,105\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 27.13\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 28.48\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 159105, 'trainable_params': 159105}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3 = dcn2()\r\n",
    "\r\n",
    "model3 = paddle.Model(cnn3)\r\n",
    "\r\n",
    "model3.summary((64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dcn3(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(dcn3, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding = 1)\r\n",
    "        # self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3),  stride=2, padding = 0)\r\n",
    "        # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "        \r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1      [[64, 3, 32, 32]]     [64, 32, 32, 32]         896      \n",
      "   Conv2D-2      [[64, 32, 32, 32]]    [64, 64, 15, 15]       18,496     \n",
      "   Conv2D-3      [[64, 64, 15, 15]]     [64, 64, 7, 7]        36,928     \n",
      "   Conv2D-4       [[64, 64, 7, 7]]      [64, 64, 4, 4]        36,928     \n",
      "   Flatten-1      [[64, 64, 4, 4]]        [64, 1024]             0       \n",
      "   Linear-1         [[64, 1024]]           [64, 64]           65,600     \n",
      "   Linear-2          [[64, 64]]            [64, 1]              65       \n",
      "===========================================================================\n",
      "Total params: 158,913\n",
      "Trainable params: 158,913\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 25.59\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 26.95\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 158913, 'trainable_params': 158913}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn4 = dcn3()\r\n",
    "\r\n",
    "model4 = paddle.Model(cnn4)\r\n",
    "\r\n",
    "model4.summary((64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Meta-ACON\n",
    "\n",
    "\n",
    "\n",
    "前面我们有提到，ACON系列的激活函数通过$\\beta$的值来控制是否激活神经元（$\\beta$ 为0，即不激活）。因此我们需要为ACON设计一个计算 $\\beta$ 的自适应函数:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8e9dc6405c8e4d598b322cbcdc40f9354e0b9e0489914e8a8099fa951f1d444e\"></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aad65672f1c842d78307506774cbc4839a0dde295f2f483f8c040159a61ff8e3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle import nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle import ParamAttr\r\n",
    "from paddle.regularizer import L2Decay\r\n",
    "from paddle.nn import AvgPool2D, Conv2D\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "class MetaAconC(nn.Layer):\r\n",
    "    r\"\"\" ACON activation (activate or not).\r\n",
    "    # MetaAconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is generated by a small network\r\n",
    "    # according to \"Activate or Not: Learning Customized Activation\" <https://arxiv.org/pdf/2009.04759.pdf>.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, width, r=16):\r\n",
    "        super().__init__()\r\n",
    "        self.fc1 = nn.Conv2D(width, max(r, width // r), kernel_size=1, stride=1)\r\n",
    "        self.bn1 = nn.BatchNorm2D(max(r, width // r))\r\n",
    "        self.fc2 = nn.Conv2D(max(r, width // r), width, kernel_size=1, stride=1)\r\n",
    "        self.bn2 = nn.BatchNorm2D(width)\r\n",
    "\r\n",
    "        self.p1 = paddle.create_parameter([1, width, 1, 1], dtype='float32', default_initializer=nn.initializer.Normal())\r\n",
    "        self.p2 = paddle.create_parameter([1, width, 1, 1], dtype='float32', default_initializer=nn.initializer.Normal())\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        beta = F.sigmoid(\r\n",
    "            self.bn2(self.fc2(self.bn1(self.fc1(x.mean(axis=2, keepdim=True).mean(axis=3, keepdim=True))))))\r\n",
    "            # self.bn2(self.fc2(self.bn1(self.fc1(x.mean().mean())))))\r\n",
    "        return (self.p1 * x - self.p2 * x) * F.sigmoid(beta * (self.p1 * x - self.p2 * x)) + self.p2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class dcn2(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(dcn2, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding = 1)\r\n",
    "        # self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3),  stride=2, padding = 0)\r\n",
    "        # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "\r\n",
    "        self.acon1 = MetaAconC(64)\r\n",
    "      \r\n",
    "\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool2(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = self.acon1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        \r\n",
    "        \r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn3 = dcn2()\r\n",
    "\r\n",
    "model3 = paddle.Model(cnn3)\r\n",
    "\r\n",
    "model3.summary((64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/6b93219550d8468bbd74e44bd647e16f73f0c81c586f4f659dd27e05d13f223a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结\n",
    "\n",
    "本教程主要关注于程序的具体复现，该激活函数的效果以及在各类计算机视觉任务上均未进行验证，大家可以根据自己的实际需求进行使用。对比二者的网络参数，使用了ACON-C激活函数会增加网络的参数量。在论文中提到的Meta-ACON将在下一个教程中为大家进行讲解。在本教程中，已经为大家演示了该激活函数的复现代码，以及如何应用在网络结构中。大家可以即插即用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
