{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 【动手学Paddle2.0系列】DropBlock理论与实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.DropBlock理论介绍\n",
    "\n",
    "\n",
    "&emsp;&emsp;dropout被广泛地用作全连接层的正则化技术，但是对于卷积层，通常不太有效。dropout在卷积层不work的原因可能是由于卷积层的特征图中相邻位置元素在空间上共享语义信息，所以尽管某个单元被dropout掉，但与其相邻的元素依然可以保有该位置的语义信息，信息仍然可以在卷积网络中流通。因此，针对卷积网络，我们需要一种结构形式的dropout来正则化，即按块来丢弃。在本文中，我们引入DropBlock，这是一种结构化的dropout形式，它将feature map相邻区域中的单元放在一起drop掉。\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;dropout的主要缺点是它随机drop特征。虽然这对于全连接层是有效的，但是对于卷积层则是无效的，因为卷积层的特征在空间上是相关的。当这些特性相互关联时，即使有dropout，有关输入的信息仍然可以发送到下一层，这会导致网络overfit。\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bb193027271b405f8d60774f867c8d4603b4271473a94e9a847b945d83d94d55\" width = \"800\"></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "**(a)原始输入图像**\n",
    "\n",
    "\n",
    "**(b)绿色部分表示激活的特征单元，b图表示了随机dropout激活单元，但是这样dropout后，网络还会从drouout掉的激活单元附近学习到同样的信息**\n",
    "\n",
    "\n",
    "**(c)绿色部分表示激活的特征单元，c图表示本文的DropBlock，通过dropout掉一部分相邻的整片的区域（比如头和脚），网络就会去注重学习狗的别的部位的特征，来实现正确分类，从而表现出更好的泛化。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.DropBlock代码实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "# 导入相关库\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.vision.transforms import ToTensor\r\n",
    "from paddle import fluid\r\n",
    "import paddle.nn as nn\r\n",
    "from paddle.fluid.optimizer import ExponentialMovingAverage\r\n",
    "\r\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = ToTensor()\r\n",
    "cifar10_train = paddle.vision.datasets.Cifar10(mode='train',\r\n",
    "                                               transform=transform)\r\n",
    "cifar10_test = paddle.vision.datasets.Cifar10(mode='test',\r\n",
    "                                              transform=transform)\r\n",
    "\r\n",
    "# 构建训练集数据加载器\r\n",
    "train_loader = paddle.io.DataLoader(cifar10_train, batch_size=64, shuffle=True)\r\n",
    "\r\n",
    "# 构建测试集数据加载器\r\n",
    "test_loader = paddle.io.DataLoader(cifar10_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DropBlock\r\n",
    "\r\n",
    "class DropBlock(nn.Layer):\r\n",
    "    def __init__(self, block_size, keep_prob, name):\r\n",
    "        super(DropBlock, self).__init__()\r\n",
    "        self.block_size = block_size\r\n",
    "        self.keep_prob = keep_prob\r\n",
    "        self.name = name\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        if not self.training or self.keep_prob == 1:\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            gamma = (1. - self.keep_prob) / (self.block_size**2)\r\n",
    "            for s in x.shape[2:]:\r\n",
    "                gamma *= s / (s - self.block_size + 1)\r\n",
    "\r\n",
    "            matrix = paddle.cast(paddle.rand(x.shape, x.dtype) < gamma, x.dtype)\r\n",
    "            mask_inv = F.max_pool2d(\r\n",
    "                matrix, self.block_size, stride=1, padding=self.block_size // 2)\r\n",
    "            mask = 1. - mask_inv\r\n",
    "            y = x * mask * (mask.numel() / mask.sum())\r\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义模型\r\n",
    "\r\n",
    "class MyNet_drop(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=10):\r\n",
    "        super(MyNet_drop, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding = 1)\r\n",
    "        # self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3),  stride=2, padding = 0)\r\n",
    "        # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "        self.DropBlock =  DropBlock(block_size=5, keep_prob=0.9, name='le')\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool2(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "        \r\n",
    "        x = self.DropBlock(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-5      [[64, 3, 32, 32]]     [64, 32, 32, 32]         896      \n",
      "   Conv2D-6      [[64, 32, 32, 32]]    [64, 64, 15, 15]       18,496     \n",
      "   Conv2D-7      [[64, 64, 15, 15]]     [64, 64, 7, 7]        36,928     \n",
      "  DropBlock-2     [[64, 64, 7, 7]]      [64, 64, 7, 7]           0       \n",
      "   Conv2D-8       [[64, 64, 7, 7]]      [64, 64, 4, 4]        36,928     \n",
      "   Flatten-4      [[64, 64, 4, 4]]        [64, 1024]             0       \n",
      "   Linear-3         [[64, 1024]]           [64, 64]           65,600     \n",
      "   Linear-4          [[64, 64]]            [64, 10]             650      \n",
      "===========================================================================\n",
      "Total params: 159,498\n",
      "Trainable params: 159,498\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 27.13\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 28.49\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 159498, 'trainable_params': 159498}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可视化模型\r\n",
    "\r\n",
    "cnn1 = MyNet_drop()\r\n",
    "\r\n",
    "model1 = paddle.Model(cnn1)\r\n",
    "\r\n",
    "model1.summary((64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/3\n",
      "step 782/782 [==============================] - loss: 1.2412 - acc: 0.4302 - 123ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 1.0604 - acc: 0.5404 - 37ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 2/3\n",
      "step 782/782 [==============================] - loss: 1.2764 - acc: 0.5712 - 121ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 0.7736 - acc: 0.6277 - 37ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 3/3\n",
      "step 782/782 [==============================] - loss: 1.1458 - acc: 0.6297 - 122ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 0.5729 - acc: 0.6558 - 36ms/step        \n",
      "Eval samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from paddle.metric import Accuracy\r\n",
    "# 配置模型\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model1.parameters())\r\n",
    "\r\n",
    "model1.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "\r\n",
    "# 模型训练与评估\r\n",
    "model1.fit(train_loader,\r\n",
    "        test_loader,\r\n",
    "        epochs=3,\r\n",
    "        verbose=1,\r\n",
    "        )\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 对比实验\n",
    "\n",
    "\n",
    "对比实验在网络结构中不使用DropBlock模块，作为对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义模型\r\n",
    "\r\n",
    "class MyNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=10):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding = 1)\r\n",
    "        # self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3),  stride=2, padding = 0)\r\n",
    "        # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "        # self.DropBlock =  DropBlock(block_size=5, keep_prob=0.9, name='le')\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool1(x)\r\n",
    "        # print(x.shape)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.pool2(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "        \r\n",
    "        # x = self.DropBlock(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1      [[64, 3, 32, 32]]     [64, 32, 32, 32]         896      \n",
      "   Conv2D-2      [[64, 32, 32, 32]]    [64, 64, 15, 15]       18,496     \n",
      "   Conv2D-3      [[64, 64, 15, 15]]     [64, 64, 7, 7]        36,928     \n",
      "   Conv2D-4       [[64, 64, 7, 7]]      [64, 64, 4, 4]        36,928     \n",
      "   Flatten-1      [[64, 64, 4, 4]]        [64, 1024]             0       \n",
      "   Linear-1         [[64, 1024]]           [64, 64]           65,600     \n",
      "   Linear-2          [[64, 64]]            [64, 10]             650      \n",
      "===========================================================================\n",
      "Total params: 159,498\n",
      "Trainable params: 159,498\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 25.60\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 26.96\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 159498, 'trainable_params': 159498}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可视化模型\r\n",
    "\r\n",
    "cnn2 = MyNet()\r\n",
    "\r\n",
    "model2 = paddle.Model(cnn2)\r\n",
    "\r\n",
    "model2.summary((64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 782/782 [==============================] - loss: 1.6163 - acc: 0.4633 - 113ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 1.6008 - acc: 0.5512 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 2/3\n",
      "step 782/782 [==============================] - loss: 0.9042 - acc: 0.6083 - 112ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 1.3076 - acc: 0.6223 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 3/3\n",
      "step 782/782 [==============================] - loss: 0.3164 - acc: 0.6757 - 115ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 157/157 [==============================] - loss: 0.6358 - acc: 0.6442 - 36ms/step        \n",
      "Eval samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# 配置模型\r\n",
    "\r\n",
    "from paddle.metric import Accuracy\r\n",
    "\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model2.parameters())\r\n",
    "\r\n",
    "model2.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "\r\n",
    "# 模型训练与评估\r\n",
    "model2.fit(train_loader,\r\n",
    "        test_loader,\r\n",
    "        epochs=3,\r\n",
    "        verbose=1,\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结\n",
    "\n",
    "\n",
    "&emsp;&emsp;本来我的想法是对比的重点是网络参数量与模型大小，不对二者的精度做对比。但经过模型的可视化之后，我发现二者的参数量和模型大小都是相同的。所以，DropBlock只是一种模型正则化方法，它不会带来任何参数量的增加，但是在大模型上能够有效的提升模型的泛化能力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
